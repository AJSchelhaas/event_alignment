{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Master_Thesis_Main_Model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP1oWsrLe6ORxGC5SPuLy9+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdoEJmrZYt6f","executionInfo":{"status":"ok","timestamp":1628189868292,"user_tz":-120,"elapsed":202702,"user":{"displayName":"Hare Software","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZq0nxV_6456ALiE0wC7nLd5bvHxU6WklnYPKikg=s64","userId":"18262664904930624910"}},"outputId":"bec47841-a0d6-46b9-9ddd-4cdcf6787a94"},"source":["import os\n","\n","# Install multilingual Roberta\n","!pip install -U sentence-transformers\n","\n","# Flair\n","!pip install flair\n","\n","# Install BLEURT\n","!git clone https://github.com/google-research/bleurt.git\n","!pip install bleurt/\n","\n","# Install NLTK package\n","import nltk\n","nltk.download('punkt')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 2.4 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 25.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 42.2 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 45.1 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 46.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=100c2654eb97bbb41fba528bd11de2525d0d2756db73cd7282ea1e6f8ac56413\n","  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.9.1\n","Collecting flair\n","  Downloading flair-0.8.0.post1-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 5.3 MB/s \n","\u001b[?25hCollecting mpld3==0.3\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[K     |████████████████████████████████| 788 kB 37.9 MB/s \n","\u001b[?25hCollecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n","Collecting segtok>=1.5.7\n","  Downloading segtok-1.5.10.tar.gz (25 kB)\n","Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 23.6 MB/s \n","\u001b[?25hCollecting sqlitedict>=1.6.0\n","  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n","Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n","Collecting konoha<5.0.0,>=4.0.0\n","  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n","Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n","Collecting deprecated>=1.2.4\n","  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n","Collecting gdown==3.12.2\n","  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.12)\n","Collecting sentencepiece==0.1.95\n","  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 31.0 MB/s \n","\u001b[?25hCollecting torch<=1.7.1,>=1.5.0\n","  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n","\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n","Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.9.1)\n","Collecting bpemb>=0.3.2\n","  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n","Collecting janome\n","  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n","\u001b[K     |████████████████████████████████| 19.7 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n","Collecting requests\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 911 kB/s \n","\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n","  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n","Collecting overrides<4.0.0,>=3.0.0\n","  Downloading overrides-3.1.0.tar.gz (11 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (5.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n","Building wheels for collected packages: gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9705 sha256=740fb85b41cb21e16bbcfb1b2b461b2cfbeefa5cbaaa20fd13328e86713620a9\n","  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116701 sha256=d2ead6dde5457afcad254558a04853cf43a24a8335febf27633c45f214b05b2d\n","  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10188 sha256=7306fea1af48aee3ac5b25d756343e67f7009248f96befdc15b5aa42d5cd4bfe\n","  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=eb916c9a3eb082da0805ac03b15bb129501be5c419b2c8d0d92106a63fe7f1e2\n","  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=45ab5f49b47d8e8b4cf175d2fda00443487152348e786d0493c9e7e8c7cfea95\n","  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=89c3401cb73d4c7fc0fd0c74fbde99ff032066c92b42ce51d53344f10b9f788c\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=7b6e61804fe59648038b4a4c68b5a9fefce23e6c61cb28640760652d781e10d2\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built gdown mpld3 overrides segtok sqlitedict ftfy langdetect\n","Installing collected packages: requests, importlib-metadata, sentencepiece, overrides, torch, sqlitedict, segtok, mpld3, langdetect, konoha, janome, gdown, ftfy, deprecated, bpemb, flair\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.6.1\n","    Uninstalling importlib-metadata-4.6.1:\n","      Successfully uninstalled importlib-metadata-4.6.1\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.96\n","    Uninstalling sentencepiece-0.1.96:\n","      Successfully uninstalled sentencepiece-0.1.96\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 3.6.4\n","    Uninstalling gdown-3.6.4:\n","      Successfully uninstalled gdown-3.6.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 requests-2.26.0 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 torch-1.7.1\n","Cloning into 'bleurt'...\n","remote: Enumerating objects: 102, done.\u001b[K\n","remote: Counting objects: 100% (62/62), done.\u001b[K\n","remote: Compressing objects: 100% (48/48), done.\u001b[K\n","remote: Total 102 (delta 20), reused 42 (delta 14), pack-reused 40\u001b[K\n","Receiving objects: 100% (102/102), 31.26 MiB | 23.69 MiB/s, done.\n","Resolving deltas: 100% (24/24), done.\n","Processing ./bleurt\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.4.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (2.5.0)\n","Collecting tf-slim>=1.1\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |████████████████████████████████| 352 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (0.1.95)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (0.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf-slim>=1.1->BLEURT==0.0.2) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2018.9)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.5.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.1.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.5.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.7.4.3)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.12)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.1.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.36.2)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.5.0.dev2021032900)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.17.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.12.1)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.34.1)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->BLEURT==0.0.2) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (3.3.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (1.32.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (57.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (0.4.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (2.26.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (1.0.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (3.10.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (0.4.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (2.0.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (2021.5.30)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow->BLEURT==0.0.2) (3.5.0)\n","Building wheels for collected packages: BLEURT\n","  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16454023 sha256=9fb83e86722fb0df4c21da2253f1156f919c5707aa9b08b7cfc73f4a18531cb2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-h261baqe/wheels/15/d1/f6/2ef9b6f78af64ad606636904e6bdc4cfae694df53f1f6f887d\n","Successfully built BLEURT\n","Installing collected packages: tf-slim, BLEURT\n","Successfully installed BLEURT-0.0.2 tf-slim-1.1.0\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7-AS_4lbPAf","executionInfo":{"status":"ok","timestamp":1628190248993,"user_tz":-120,"elapsed":175111,"user":{"displayName":"Hare Software","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZq0nxV_6456ALiE0wC7nLd5bvHxU6WklnYPKikg=s64","userId":"18262664904930624910"}},"outputId":"3f5c158d-1610-471e-aa6b-426f5462251e"},"source":["import os\n","import sys\n","import nltk\n","import json\n","import csv \n","import operator\n","import logging\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity\n","from bleurt import score\n","from flair.data import Sentence\n","from flair.models import SequenceTagger\n","\n","# tf.get_logger().setLevel(logging.ERROR)\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","logging.getLogger('tensorflow').disabled = True\n","\n","# mount google drive\n","drive.mount('/content/gdrive')\n","\n","# define taggers (uncomment if you want to use Flair NER taggers)\n","tagger_NL = None\n","tagger_DE = None\n","tagger_EN = None\n","\n","'''\n","tagger_NL = SequenceTagger.load(\"flair/ner-dutch\")\n","tagger_DE = SequenceTagger.load(\"flair/ner-german\")\n","tagger_EN = SequenceTagger.load(\"flair/ner-english\")\n","'''\n","\n","def get_similarity(sum, rep):\n","    # put all sentence embeddings in a matrix\n","    e_col = 'sentence_embedding_bert'\n","\n","    embed_mat_sum = sum\n","    embed_mat_rep = rep\n","\n","    # calculate distance between every embedding pair\n","    sim_mat = cosine_similarity(embed_mat_sum, embed_mat_rep)\n","\n","    return sim_mat\n","\n","def get_similarity_bleurt(rep_sentence, sum_sentences_index, df_sum_text):\n","    sim_mat = []\n","\n","    for sum_sent_index in sum_sentences_index:\n","        sum_sent = df_sum_text[sum_sent_index]\n","        references = tf.constant([rep_sentence])\n","        candidates = tf.constant([sum_sent])\n","\n","        # Perform scoring\n","        bleurt_ops = score.create_bleurt_ops()\n","        bleurt_out = bleurt_ops(references=references, candidates=candidates)\n","        assert bleurt_out[\"predictions\"].shape == (1,)\n","\n","        # Print results\n","        bleurt_score = bleurt_out[\"predictions\"].numpy()[0]\n","        sim_mat.append(bleurt_score)\n","\n","    return sim_mat\n","\n","\n","def get_similarity_bleurt_string(sentence_1, sentence_2):\n","    references = tf.constant([sentence_1])\n","    candidates = tf.constant([sentence_2])\n","\n","    # Perform scoring\n","    bleurt_ops = score.create_bleurt_ops()\n","    bleurt_out = bleurt_ops(references=references, candidates=candidates)\n","    assert bleurt_out[\"predictions\"].shape == (1,)\n","\n","    # Print results\n","    bleurt_score = bleurt_out[\"predictions\"].numpy()[0]\n","\n","    return bleurt_score\n","\n","\n","def get_similarity_embeddings_string(sentence_1, sentence_2, embedding_model):\n","    df_sentence_1 = pd.DataFrame({\"text\": [sentence_1]})\n","    df_sentence_2 = pd.DataFrame({\"text\": [sentence_2]})\n","    embeddings_sentence_1 = embedding_model.encode(df_sentence_1.text)\n","    embeddings_sentence_2 = embedding_model.encode(df_sentence_2.text)\n","\n","    sim_mat = cosine_similarity(embeddings_sentence_1, embeddings_sentence_2)[0][0]\n","\n","    return sim_mat\n","\n","\n","def get_entities(json_path):\n","    with open(json_path) as f:\n","        match_data = json.load(f)\n","        \n","    entity_counter = 0\n","    entity_dic = {}\n","\n","    for event in match_data:\n","        if 'primaryPlayer' in event:\n","            first_name = None;\n","            last_name = None;\n","            position = None;\n","\n","            # Get player and ID\n","            player = event['primaryPlayer']\n","            id = player['id']\n","\n","            # Check if already exists\n","            if id in entity_dic:\n","                entry = entity_dic[id]\n","                first_name = entry[0];\n","                last_name = entry[1];\n","                #position = entry[2];               \n","\n","            # First name\n","            if 'displayFirstName' in player and first_name == None:\n","                first_name = player['displayFirstName']\n","\n","            # Last name\n","            if 'displayLastName' in player and last_name == None:\n","                last_name = player['displayLastName']\n","\n","            # Position\n","            if 'fieldPosition' in player and position == None:\n","                position = player['fieldPosition']\n","\n","            # Create entity\n","            entity_dic[id] = (first_name, last_name)\n","\n","    return entity_dic\n","\n","def get_sentence_entities(sentence, entity_dic):\n","    tokens = nltk.word_tokenize(sentence)\n","    tokens = [x.lower() for x in tokens]\n","    entity_list = []\n","\n","    for entity_id, entity_tuple in entity_dic.items():\n","        for entity_part in entity_tuple:\n","            if entity_part != None and entity_part.lower() in tokens:\n","                entity_list.append(entity_id)\n","\n","    return entity_list\n","\n","def get_sentence_time(sentence):\n","    sentence_minute = None\n","\n","    # First detect whether \"min\" appears in sentence\n","    tokens = nltk.word_tokenize(sentence)\n","    tokens = [x.lower() for x in tokens]\n","\n","    for token_index, token in enumerate(tokens):\n","        max_index = len(tokens)-1\n","        if \"min\" in token:\n","            lower_index = token_index - 2\n","            upper_index = 1 + token_index + 2\n","            if lower_index < 0:\n","                lower_index = 0\n","            if upper_index > max_index:\n","                upper_index = max_index\n","\n","            token_selection = tokens[lower_index:upper_index]\n","            for current_token in token_selection:\n","                if current_token.isnumeric():\n","                    sentence_minute = int(current_token)\n","\n","    return sentence_minute\n","\n","\n","def get_bleurt_score_for_set(rep_sent, sum_sent_index_set, df_sum_text):\n","    total_score = 0\n","    for sum_sent_index in sum_sent_index_set:\n","        sum_sent = df_sum_text[sum_sent_index]\n","        references = tf.constant([rep_sent])\n","        candidates = tf.constant([sum_sent])\n","\n","        # Perform scoring\n","        bleurt_ops = score.create_bleurt_ops()\n","        bleurt_out = bleurt_ops(references=references, candidates=candidates)\n","        assert bleurt_out[\"predictions\"].shape == (1,)\n","\n","        # Get result\n","        bleurt_score = bleurt_out[\"predictions\"].numpy()[0]\n","\n","        # Add result\n","        total_score += bleurt_score\n","\n","    return total_score\n","\n","\n","def get_NER_tags(sentence_string, tagger_model):\n","    sentence = Sentence(sentence_string)\n","    tagger_model.predict(sentence)\n","\n","    ner_entity_list = []\n","    for entity in sentence.to_dict('ner')['entities']:\n","        ner_entity_list.append(entity)\n","\n","    return ner_entity_list\n","\n","\n","def check_NER_tags(ner_entity_list):\n","    person_present = False\n","    for entity in ner_entity_list:\n","        if entity['labels'][0].value == 'PER':\n","            person_present = True\n","\n","    return person_present\n","\n","\n","def save_annotations(annotation_file, annotation_dict):\n","    lines = []\n","    for key, values in annotation_dict.items():\n","        # 0 - match_id\n","        # 1 - report_id_1\n","        # 2 - report_sent_1\n","        # 3 - report_id_2\n","        # 4 - report_sent_2\n","        lines.append('{}\\t{}\\t\"{}\"\\t{}\\t\"{}\"\\n'.format(key, values[0], values[1], values[2], values[3]))\n","    \n","    with open(annotation_file, \"w\", encoding='utf-8') as f:\n","        f.writelines(lines)\n","\n","\n","def main():\n","    # Model parameters, feel free to change if required.\n","    summary_sentence_amount = 10\n","    summary_sentence_match_minimum = 3\n","\n","    # Whether to use embeddings or BLEURT for similarity scoring\n","    use_embeddings_for_scoring = False\n","\n","    # Change this to the match id that you want to extract alignment pairs from\n","    match_id_input = \"2002833\"\n","\n","    # Load Roberta multilingual\n","    from sentence_transformers import SentenceTransformer\n","    embedding_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n","\n","    # Load data from Google Drive (Change these paths to where your match report dataset and structured data files are)\n","    txt_directory = \"/content/gdrive/MyDrive/Master_Scriptie/colab/match_reports_dataset\"\n","    sum_directory = \"/content/gdrive/MyDrive/Master_Scriptie/colab/events_all\"\n","    for dirname in os.listdir(txt_directory):\n","        match_id = dirname\n","        if match_id == match_id_input:\n","            print()\n","            print(\"Match ID:\", match_id)\n","\n","            # get file paths\n","            txt_file_dir = os.path.join(txt_directory, dirname)\n","            json_file = os.path.join(sum_directory, match_id + \".json\")\n","            csv_file = os.path.join(sum_directory, match_id + \"_summary.csv\")\n","            \n","            # retrieve entities and ID's\n","            entity_dic = get_entities(json_file)\n","\n","            # get dataframe\n","            df_sum = pd.read_csv(csv_file)\n","\n","            # get entities in a sentence\n","            entity_list_column = []\n","            for sentence in df_sum.autoText:\n","                entity_list = get_sentence_entities(sentence, entity_dic)\n","                entity_list_column.append(entity_list)\n","            df_sum[\"entity_list\"] = entity_list_column\n","            df_sum.rename(columns = {'autoText':'text'}, inplace = True)\n","\n","            # get summary embeddings\n","            summary_embeddings = embedding_model.encode(df_sum.text)\n","\n","            # Save each report sentence with the sum_sent_index\n","            rep_sent_list = []\n","\n","            # Go over each report for a match\n","            all_report_enabled = True\n","            for report_name in os.listdir(txt_file_dir):\n","                # go over single file\n","                report_path = os.path.join(txt_file_dir, report_name)\n","                with open(report_path, \"r\", encoding=\"UTF-8\") as f:\n","                    # Get report language\n","                    tagger_model = tagger_EN\n","                    if \"NL-\" in report_path:\n","                        tagger = tagger_NL\n","                    if \"DE-\" in report_path:\n","                        tagger = tagger_DE\n","\n","                    if \"EN-\" in report_path or all_report_enabled:\n","                        print(\"Report Name:\", report_name)\n","\n","                        # Get sentences\n","                        report_string = f.read()\n","                        report_sentences = nltk.sent_tokenize(report_string)\n","                        df_rep = pd.DataFrame({\"text\": report_sentences})\n","\n","                        for rep_sent in report_sentences:\n","                            get_sentence_time(rep_sent)\n","\n","                        # Get embeddings\n","                        report_embeddings = embedding_model.encode(df_rep.text)\n","\n","                        # Get embedding similarity\n","                        sim_mat = get_similarity(report_embeddings, summary_embeddings)\n","                        sim_mat_rep = get_similarity(report_embeddings, report_embeddings)\n","\n","                        # Go over every sentence in report\n","                        for rep_sent_index, rep_sent_row in enumerate(sim_mat):\n","                            # Get report sentence\n","                            rep_sent = df_rep.text[rep_sent_index]\n","                            rep_sent_minute = get_sentence_time(rep_sent)\n","\n","                            # set summary sentence index set\n","                            rep_sent_sum_sent_index_set = set()\n","\n","                            # check if the sentence has any NER person tags\n","                            NER_disabled = True\n","                            ner_entity_list = []\n","                            if not NER_disabled:\n","                                ner_entity_list = get_NER_tags(rep_sent, tagger_model)\n","\n","                            if check_NER_tags(ner_entity_list) or NER_disabled:\n","                                # Check for entities\n","                                rep_entity_list = get_sentence_entities(rep_sent, entity_dic)\n","\n","                                # Get highes indices for a report sentence\n","                                n_amount = 10\n","                                highest_cos_score_list = list(reversed(sorted(range(len(rep_sent_row)), key=lambda i: rep_sent_row[i])[-n_amount:]))\n","\n","                                # Get entities for summary sentence and match accordingly\n","                                for index in highest_cos_score_list:\n","                                    sum_sent = df_sum.text[index]\n","                                    sum_minute = df_sum.minute[index]\n","\n","                                    minutes_passed = 120\n","                                    if (rep_sent_minute != None):\n","                                        minutes_passed = rep_sent_minute\n","\n","                                    sum_entity_list = get_sentence_entities(sum_sent, entity_dic)\n","                                    # Check which sum sentence contains report entities\n","                                    for rep_entity in rep_entity_list:\n","                                        if (rep_entity in sum_entity_list) and (sum_minute <= minutes_passed):\n","                                            rep_sent_sum_sent_index_set.add(index)\n","\n","                                    # Stop adding new indices when 3 have already been added\n","                                    if len(rep_sent_sum_sent_index_set) >= summary_sentence_amount:\n","                                        break\n","\n","                            # Save report sentence with sum_sentences_index\n","                            rep_sent_tuple = (rep_sent, rep_sent_sum_sent_index_set, report_name, rep_sent_index)\n","                            rep_sent_list.append(rep_sent_tuple)\n","\n","            # Get report sentences with most similar sum sentences\n","            matched_report_sentences = {}\n","\n","            for rep_sent_tuple in rep_sent_list:\n","                # Extract sentence information\n","                rep_sent_string = rep_sent_tuple[0]\n","                rep_sent_sum_set = rep_sent_tuple[1]\n","                rep_sent_report_name = rep_sent_tuple[2]\n","                rep_sent_index = rep_sent_tuple[3]\n","\n","                # Highest score init\n","                rep_sent_highest_score = None\n","                rep_sent_highest_score_tuple = None\n","\n","                rep_sent_score_list = []\n","                \n","                # Compare report sentence to all other report sentences and find most common one\n","                for rep_sent_tuple_compare in rep_sent_list:\n","                    rep_sent_compare_string = rep_sent_tuple_compare[0]\n","                    rep_sent_compare_sum_set = rep_sent_tuple_compare[1]\n","                    rep_sent_compare_report_name = rep_sent_tuple_compare[2]\n","                    rep_sent_compare_index = rep_sent_tuple[3]\n","\n","                    if rep_sent_report_name != rep_sent_compare_report_name:\n","                        # Only allow sentences that have the minimum of required matches\n","                        set_intersection = rep_sent_sum_set & rep_sent_compare_sum_set\n","                        set_intersection_len = len(set_intersection)\n","                        if set_intersection_len > summary_sentence_match_minimum:\n","                            # calculate the bleurt score for the intersection to allow comparision with other matches\n","                            if use_embeddings_for_scoring:\n","                                total_score = get_similarity_embeddings_string(rep_sent_string, rep_sent_compare_string, embedding_model)\n","                            else:\n","                                total_score = get_similarity_bleurt_string(rep_sent_string, rep_sent_compare_string)\n","\n","                            ann_id = \"{}_{}_{}_{}\".format(rep_sent_tuple[2], rep_sent_tuple[3], rep_sent_tuple_compare[2], rep_sent_tuple_compare[3])\n","                            ann_report_1 = rep_sent_tuple[2]\n","                            ann_sent_1 = rep_sent_tuple[0]\n","                            ann_report_2 = rep_sent_tuple_compare[2]\n","                            ann_sent_2 = rep_sent_tuple_compare[0]\n","                            matched_report_sentence = [ann_report_1, ann_sent_1, ann_report_2, ann_sent_2]\n","\n","                            rep_sent_score_list.append((total_score, matched_report_sentence, ann_id))\n","\n","                            if (rep_sent_highest_score == None or total_score > rep_sent_highest_score):\n","                                rep_sent_highest_score = total_score\n","                                rep_sent_highest_score_tuple = rep_sent_tuple_compare\n","\n","\n","                # Sort list with scores\n","                rep_sent_score_list.sort(key=lambda x:x[0])\n","                rep_sent_score_list.reverse()\n","\n","                # Add top matches\n","                k_amount = 5\n","                for score_tuple in rep_sent_score_list[:k_amount]:\n","                    matched_report_sentence = score_tuple[1]\n","                    ann_id = score_tuple[2]\n","                    matched_report_sentences[ann_id] = matched_report_sentence\n","\n","            # Print matched report sentences\n","            scoring_name_list = [\"bleurt\", \"embeddings\"]\n","            pred_file_name = \"{}_pred_{}_{}.tsv\".format(match_id, scoring_name_list[use_embeddings_for_scoring], k_amount)\n","            save_annotations(pred_file_name, matched_report_sentences)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\n","Match ID: 2002833\n","Report Name: EN-2.txt\n","Report Name: NL-2.txt\n","Report Name: EN-3.txt\n","Report Name: NL-1.txt\n","Report Name: NL-3.txt\n","Report Name: DE-1.txt\n","Report Name: DE-3.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F_EdzNVPkYEv"},"source":[""],"execution_count":null,"outputs":[]}]}